"""Implementation of the paper [1].

[1] G. Trigeorgis et al., 'Adieu features? End-to-end speech emotion
recognition using a deep convolutional recurrent network', in 2016 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), Shanghai, 2016, pp. 5200â€“5204, doi:
10.1109/ICASSP.2016.7472669.
"""

import numpy as np
import pandas as pd
import soundfile
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import (
    RNN,
    Bidirectional,
    Conv1D,
    Dense,
    Dropout,
    Flatten,
    GaussianNoise,
    Input,
    LSTMCell,
    MaxPool1D,
    Reshape,
    TimeDistributed,
)
from tensorflow.keras.losses import Loss
from tensorflow.keras.metrics import MeanSquaredError
from tensorflow.keras.optimizers import Adam


class CCCLoss(Loss):
    def call(self, y_true, y_pred):
        mu_x = tf.reduce_mean(y_true, -1, name="mu_x")
        var_x = tf.math.reduce_variance(y_true, -1, name="var_x")
        mu_y = tf.reduce_mean(y_pred, -1, name="mu_y")
        var_y = tf.math.reduce_variance(y_pred, -1, name="var_y")

        phi = var_x + var_y + (mu_x - mu_y) ** 2
        phi_inv = tf.math.reciprocal_no_nan(phi)
        sxy = tf.reduce_mean(
            (y_true - mu_x[:, tf.newaxis]) * (y_pred - mu_y[:, tf.newaxis]), -1
        )
        # Negative so that we can minimise -CCC = maximise CCC
        ccc = -2 * sxy * phi_inv
        return ccc


def get_model():
    inputs = Input((96000, 1), name="input")

    noise = GaussianNoise(0.1)(inputs)
    conv1 = Conv1D(40, 80, padding="same", use_bias=False, name="conv_1")(noise)
    dropout1 = Dropout(0.5)(conv1)
    rectifier = tf.maximum(dropout1, 0, name="halfwave_rectifier")
    maxpool_time = MaxPool1D(2, name="maxpool_time")(rectifier)

    conv2 = Conv1D(40, 4000, padding="same", use_bias=False, name="conv_2")(
        maxpool_time
    )
    dropout2 = Dropout(0.5)(conv2)
    maxpool_channels = MaxPool1D(
        20, data_format="channels_first", name="maxpool_channels"
    )(dropout2)

    split = Reshape((150, 320, 2), name="split150")(maxpool_channels)

    # recurrent = Bidirectional(LSTM(128, return_sequences=True),
    #                           name='blstm_1')(maxpool_channels)
    # recurrent = Bidirectional(LSTM(128), name='blstm_2')(recurrent)

    blstm = Bidirectional(RNN([LSTMCell(128), LSTMCell(128)]), name="blstm")
    x = TimeDistributed(blstm)(split)
    x = Dense(1, activation="tanh")(x)
    x = Flatten()(x)
    return Model(inputs=inputs, outputs=x)


def main():
    for gpu in tf.config.list_physical_devices("GPU"):
        tf.config.experimental.set_memory_growth(gpu, True)

    df = pd.read_csv("datasets/semaine/combined/1/emotions.csv", header=0, index_col=0)
    df.index = pd.TimedeltaIndex(df.index, unit="s")
    df = df.resample("40ms").mean().ffill()

    audio, _ = soundfile.read("datasets/semaine/combined/1/user.wav", always_2d=True)
    arr = []
    annot = []
    for i in range(100):
        arr.append(tf.constant(audio[i * 96000 : (i + 1) * 96000]))
        time_range = slice(pd.Timedelta(i * 6, "s"), pd.Timedelta((i + 1) * 6, "s"))
        annot.append(df["Activation"][time_range].astype(np.float32))
    audio_data = tf.data.Dataset.from_tensor_slices((arr, annot))
    audio_data = audio_data.shuffle(100).batch(4)
    del arr, annot, audio, df

    model = get_model()
    model.compile(loss=CCCLoss(), metrics=[MeanSquaredError()], optimizer=Adam())
    model.summary()

    model.fit(audio_data, epochs=50, verbose=1)


if __name__ == "__main__":
    main()
